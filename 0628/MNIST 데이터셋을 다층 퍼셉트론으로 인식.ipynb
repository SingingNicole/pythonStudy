{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602e12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d001a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "mnist=fetch_openml('mnist_784')\n",
    "mnist.data=mnist.data/255.0\n",
    "x_train=mnist.data[:60000]; x_test=mnist.data[60000:]\n",
    "y_train=np.int16(mnist.target[:60000]); y_test=np.int16(mnist.target[60000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0674efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60622132\n",
      "Iteration 2, loss = 0.26428380\n",
      "Iteration 3, loss = 0.20907443\n",
      "Iteration 4, loss = 0.17411163\n",
      "Iteration 5, loss = 0.14922121\n",
      "Iteration 6, loss = 0.12984897\n",
      "Iteration 7, loss = 0.11590391\n",
      "Iteration 8, loss = 0.10286602\n",
      "Iteration 9, loss = 0.09262429\n",
      "Iteration 10, loss = 0.08447996\n",
      "Iteration 11, loss = 0.07715326\n",
      "Iteration 12, loss = 0.07079413\n",
      "Iteration 13, loss = 0.06558910\n",
      "Iteration 14, loss = 0.06038039\n",
      "Iteration 15, loss = 0.05536567\n",
      "Iteration 16, loss = 0.05138782\n",
      "Iteration 17, loss = 0.04804499\n",
      "Iteration 18, loss = 0.04429679\n",
      "Iteration 19, loss = 0.04239184\n",
      "Iteration 20, loss = 0.03910674\n",
      "Iteration 21, loss = 0.03656380\n",
      "Iteration 22, loss = 0.03372927\n",
      "Iteration 23, loss = 0.03144882\n",
      "Iteration 24, loss = 0.02948088\n",
      "Iteration 25, loss = 0.02776163\n",
      "Iteration 26, loss = 0.02561233\n",
      "Iteration 27, loss = 0.02452902\n",
      "Iteration 28, loss = 0.02264447\n",
      "Iteration 29, loss = 0.02090661\n",
      "Iteration 30, loss = 0.01962255\n",
      "Iteration 31, loss = 0.01810390\n",
      "Iteration 32, loss = 0.01691042\n",
      "Iteration 33, loss = 0.01617642\n",
      "Iteration 34, loss = 0.01479767\n",
      "Iteration 35, loss = 0.01403398\n",
      "Iteration 36, loss = 0.01296767\n",
      "Iteration 37, loss = 0.01215975\n",
      "Iteration 38, loss = 0.01113953\n",
      "Iteration 39, loss = 0.01053345\n",
      "Iteration 40, loss = 0.00959299\n",
      "Iteration 41, loss = 0.00922387\n",
      "Iteration 42, loss = 0.00855228\n",
      "Iteration 43, loss = 0.00801234\n",
      "Iteration 44, loss = 0.00754208\n",
      "Iteration 45, loss = 0.00694375\n",
      "Iteration 46, loss = 0.00667116\n",
      "Iteration 47, loss = 0.00608182\n",
      "Iteration 48, loss = 0.00577435\n",
      "Iteration 49, loss = 0.00533421\n",
      "Iteration 50, loss = 0.00480003\n",
      "Iteration 51, loss = 0.00446131\n",
      "Iteration 52, loss = 0.00423402\n",
      "Iteration 53, loss = 0.00416885\n",
      "Iteration 54, loss = 0.00375544\n",
      "Iteration 55, loss = 0.00364004\n",
      "Iteration 56, loss = 0.00326225\n",
      "Iteration 57, loss = 0.00312430\n",
      "Iteration 58, loss = 0.00314884\n",
      "Iteration 59, loss = 0.00260122\n",
      "Iteration 60, loss = 0.00254202\n",
      "Iteration 61, loss = 0.00239044\n",
      "Iteration 62, loss = 0.00226100\n",
      "Iteration 63, loss = 0.00210368\n",
      "Iteration 64, loss = 0.00204377\n",
      "Iteration 65, loss = 0.00193879\n",
      "Iteration 66, loss = 0.00184617\n",
      "Iteration 67, loss = 0.00178416\n",
      "Iteration 68, loss = 0.00166195\n",
      "Iteration 69, loss = 0.00172890\n",
      "Iteration 70, loss = 0.00142001\n",
      "Iteration 71, loss = 0.00135281\n",
      "Iteration 72, loss = 0.00139817\n",
      "Iteration 73, loss = 0.00125503\n",
      "Iteration 74, loss = 0.00118273\n",
      "Iteration 75, loss = 0.00114860\n",
      "Iteration 76, loss = 0.00107214\n",
      "Iteration 77, loss = 0.00099766\n",
      "Iteration 78, loss = 0.00214812\n",
      "Iteration 79, loss = 0.00896562\n",
      "Iteration 80, loss = 0.00431352\n",
      "Iteration 81, loss = 0.00126967\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=512, hidden_layer_sizes=100, max_iter=300,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP 분류기 모델을 학습\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=512,max_iter=300,solver='adam',verbose=True)\n",
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6eb600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.710e+02 0.000e+00 4.000e+00 0.000e+00 0.000e+00 1.000e+00 4.000e+00\n",
      "  0.000e+00 6.000e+00 2.000e+00]\n",
      " [0.000e+00 1.124e+03 3.000e+00 0.000e+00 0.000e+00 2.000e+00 2.000e+00\n",
      "  5.000e+00 0.000e+00 3.000e+00]\n",
      " [0.000e+00 3.000e+00 9.990e+02 3.000e+00 0.000e+00 0.000e+00 2.000e+00\n",
      "  8.000e+00 1.000e+00 0.000e+00]\n",
      " [1.000e+00 1.000e+00 5.000e+00 9.890e+02 1.000e+00 9.000e+00 1.000e+00\n",
      "  3.000e+00 7.000e+00 5.000e+00]\n",
      " [1.000e+00 0.000e+00 2.000e+00 0.000e+00 9.640e+02 1.000e+00 2.000e+00\n",
      "  2.000e+00 5.000e+00 7.000e+00]\n",
      " [0.000e+00 1.000e+00 0.000e+00 3.000e+00 1.000e+00 8.680e+02 6.000e+00\n",
      "  0.000e+00 4.000e+00 2.000e+00]\n",
      " [3.000e+00 2.000e+00 3.000e+00 1.000e+00 7.000e+00 3.000e+00 9.380e+02\n",
      "  0.000e+00 1.000e+00 0.000e+00]\n",
      " [1.000e+00 1.000e+00 6.000e+00 3.000e+00 1.000e+00 2.000e+00 0.000e+00\n",
      "  1.000e+03 3.000e+00 3.000e+00]\n",
      " [3.000e+00 3.000e+00 9.000e+00 5.000e+00 1.000e+00 5.000e+00 3.000e+00\n",
      "  3.000e+00 9.450e+02 3.000e+00]\n",
      " [0.000e+00 0.000e+00 1.000e+00 6.000e+00 7.000e+00 1.000e+00 0.000e+00\n",
      "  7.000e+00 2.000e+00 9.840e+02]]\n",
      "테스트 집합에 대한 정확률은  97.82 %입니다.\n"
     ]
    }
   ],
   "source": [
    "res=mlp.predict(x_test) # 테스트 집합으로 예측\n",
    "\n",
    "# 혼동 행렬\n",
    "conf=np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy=no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
