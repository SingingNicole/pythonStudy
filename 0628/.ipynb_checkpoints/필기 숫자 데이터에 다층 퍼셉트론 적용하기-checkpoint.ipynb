{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba73de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit=datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18d7070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91084422\n",
      "Iteration 2, loss = 0.29916337\n",
      "Iteration 3, loss = 0.19432951\n",
      "Iteration 4, loss = 0.14678650\n",
      "Iteration 5, loss = 0.11691804\n",
      "Iteration 6, loss = 0.09719261\n",
      "Iteration 7, loss = 0.08419944\n",
      "Iteration 8, loss = 0.07260135\n",
      "Iteration 9, loss = 0.06102944\n",
      "Iteration 10, loss = 0.05556228\n",
      "Iteration 11, loss = 0.05044168\n",
      "Iteration 12, loss = 0.04439769\n",
      "Iteration 13, loss = 0.04004974\n",
      "Iteration 14, loss = 0.03883401\n",
      "Iteration 15, loss = 0.03422785\n",
      "Iteration 16, loss = 0.03168028\n",
      "Iteration 17, loss = 0.03124596\n",
      "Iteration 18, loss = 0.02741587\n",
      "Iteration 19, loss = 0.02673268\n",
      "Iteration 20, loss = 0.02550080\n",
      "Iteration 21, loss = 0.02363433\n",
      "Iteration 22, loss = 0.02182437\n",
      "Iteration 23, loss = 0.02048912\n",
      "Iteration 24, loss = 0.02131095\n",
      "Iteration 25, loss = 0.01959406\n",
      "Iteration 26, loss = 0.01837113\n",
      "Iteration 27, loss = 0.01764438\n",
      "Iteration 28, loss = 0.01689953\n",
      "Iteration 29, loss = 0.01607463\n",
      "Iteration 30, loss = 0.01563850\n",
      "Iteration 31, loss = 0.01506091\n",
      "Iteration 32, loss = 0.01430642\n",
      "Iteration 33, loss = 0.01412431\n",
      "Iteration 34, loss = 0.01344891\n",
      "Iteration 35, loss = 0.01300329\n",
      "Iteration 36, loss = 0.01268553\n",
      "Iteration 37, loss = 0.01225942\n",
      "Iteration 38, loss = 0.01215823\n",
      "Iteration 39, loss = 0.01180779\n",
      "Iteration 40, loss = 0.01144191\n",
      "Iteration 41, loss = 0.01128384\n",
      "Iteration 42, loss = 0.01084053\n",
      "Iteration 43, loss = 0.01054205\n",
      "Iteration 44, loss = 0.01024776\n",
      "Iteration 45, loss = 0.00987583\n",
      "Iteration 46, loss = 0.00978802\n",
      "Iteration 47, loss = 0.00971976\n",
      "Iteration 48, loss = 0.00953319\n",
      "Iteration 49, loss = 0.00911683\n",
      "Iteration 50, loss = 0.00892991\n",
      "Iteration 51, loss = 0.00881638\n",
      "Iteration 52, loss = 0.00874344\n",
      "Iteration 53, loss = 0.00840921\n",
      "Iteration 54, loss = 0.00823435\n",
      "Iteration 55, loss = 0.00820876\n",
      "Iteration 56, loss = 0.00790647\n",
      "Iteration 57, loss = 0.00775930\n",
      "Iteration 58, loss = 0.00766085\n",
      "Iteration 59, loss = 0.00747528\n",
      "Iteration 60, loss = 0.00738181\n",
      "Iteration 61, loss = 0.00724735\n",
      "Iteration 62, loss = 0.00717901\n",
      "Iteration 63, loss = 0.00705426\n",
      "Iteration 64, loss = 0.00681549\n",
      "Iteration 65, loss = 0.00676130\n",
      "Iteration 66, loss = 0.00667463\n",
      "Iteration 67, loss = 0.00659145\n",
      "Iteration 68, loss = 0.00652894\n",
      "Iteration 69, loss = 0.00632993\n",
      "Iteration 70, loss = 0.00630797\n",
      "Iteration 71, loss = 0.00625382\n",
      "Iteration 72, loss = 0.00606848\n",
      "Iteration 73, loss = 0.00606265\n",
      "Iteration 74, loss = 0.00594389\n",
      "Iteration 75, loss = 0.00583280\n",
      "Iteration 76, loss = 0.00576476\n",
      "Iteration 77, loss = 0.00573687\n",
      "Iteration 78, loss = 0.00560214\n",
      "Iteration 79, loss = 0.00554449\n",
      "Iteration 80, loss = 0.00562854\n",
      "Iteration 81, loss = 0.00541712\n",
      "Iteration 82, loss = 0.00533693\n",
      "Iteration 83, loss = 0.00526870\n",
      "Iteration 84, loss = 0.00517831\n",
      "Iteration 85, loss = 0.00513253\n",
      "Iteration 86, loss = 0.00508396\n",
      "Iteration 87, loss = 0.00509796\n",
      "Iteration 88, loss = 0.00491953\n",
      "Iteration 89, loss = 0.00485150\n",
      "Iteration 90, loss = 0.00482306\n",
      "Iteration 91, loss = 0.00478850\n",
      "Iteration 92, loss = 0.00471924\n",
      "Iteration 93, loss = 0.00466848\n",
      "Iteration 94, loss = 0.00465520\n",
      "Iteration 95, loss = 0.00455356\n",
      "Iteration 96, loss = 0.00456942\n",
      "Iteration 97, loss = 0.00454855\n",
      "Iteration 98, loss = 0.00440638\n",
      "Iteration 99, loss = 0.00432806\n",
      "Iteration 100, loss = 0.00432036\n",
      "Iteration 101, loss = 0.00431360\n",
      "Iteration 102, loss = 0.00426859\n",
      "Iteration 103, loss = 0.00423807\n",
      "Iteration 104, loss = 0.00418043\n",
      "Iteration 105, loss = 0.00409977\n",
      "Iteration 106, loss = 0.00404905\n",
      "Iteration 107, loss = 0.00404470\n",
      "Iteration 108, loss = 0.00399618\n",
      "Iteration 109, loss = 0.00393611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# MLP 분류기 모델을 학습\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True)\n",
    "# learning_rate_init: 학습률\n",
    "# sgd: 경사하강법 / 조금만 보고 빨리 판단.\n",
    "# batch_size: 전체 데이터를 해당 사이즈(사용자 지정)로 나눠 배치로 학습.\n",
    "# sgd와 batch_size를 같이 사용하면 미니배치 경사하강법을 사용한 것이다. => 최적화 알고리즘\n",
    "mlp.fit(x_train,y_train)\n",
    "\n",
    "res=mlp.predict(x_test) # 테스트 집합으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d8f97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0. 78.  1.  0.  0.  0.  2.  0.  3.  1.]\n",
      " [ 1.  0. 67.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0. 56.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0. 71.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0. 79.  0.  0.  2.  0.]\n",
      " [ 1.  0.  0.  0.  1.  0. 74.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0. 67.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. 59.  3.]\n",
      " [ 0.  0.  0.  0.  0.  3.  0.  1.  0. 76.]]\n"
     ]
    }
   ],
   "source": [
    "# 혼동 행렬\n",
    "conf=np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5914a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 집합에 대한 정확률은  96.66203059805285 %입니다.\n"
     ]
    }
   ],
   "source": [
    "# 정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy=no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")\n",
    "# svm보다 열등하고 퍼셉트론보다는 우수함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
