{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec216308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = '코로나_naver_news'\n",
    "with open('./data/' + file_name + '.json', encoding='utf8') as j_f:\n",
    "    data = json.load(j_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25315cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_title = []\n",
    "data_description = []\n",
    "\n",
    "for item in data:\n",
    "    data_title.append(item['title'])\n",
    "    data_description.append(item['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625448a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a36d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame({'title':data_title, 'description':data_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7834c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['title'] = data_df['title'].apply(lambda x : re.sub(r'[^ ㄱ-ㅣ 가-힣]+', \" \", x))\n",
    "data_df['description'] = data_df['description'].apply(lambda x : re.sub(r'[^ ㄱ-ㅣ 가-힣]+', \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bef2548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>당근알바  동네 가게 사장님 위한  알바 구인 이벤트</td>\n",
       "      <td>사장님들이 당근알바를 이용해 동네에서 빠르게 필요한 일손을 구하고 당근알바가 준비한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>자영업자 연체율  로 상승  년내 최고치 다중채무자 비중   달해</td>\n",
       "      <td>대출잔액  조원 사상 최대 기록 경신  금융 연체율   상승 저축은행   최근 수년...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>게시판    글로벌 바이오 교육생  차바이오그룹 견학</td>\n",
       "      <td>글로벌 바이오 인력양성 허브는  코로나 를 계기로 지역별 백신 바이오의약품 생산...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배위량 선교사의 첫 걸음으로 이룬  안동지역 근대화  조명</td>\n",
       "      <td>한편 김승학 목사는 이번 포럼과 관련해  배위량 선교사의 첫 발걸음이 단순히 교회 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>화장품  중국   전략 재검토 및 수정 시급</td>\n",
       "      <td>그러나  년전에는  코로나 가 퍼지고 경제가 침체된  년이었고  행사의 부진은 당연...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "0        당근알바  동네 가게 사장님 위한  알바 구인 이벤트    \n",
       "1  자영업자 연체율  로 상승  년내 최고치 다중채무자 비중   달해   \n",
       "2         게시판    글로벌 바이오 교육생  차바이오그룹 견학   \n",
       "3      배위량 선교사의 첫 걸음으로 이룬  안동지역 근대화  조명   \n",
       "4              화장품  중국   전략 재검토 및 수정 시급   \n",
       "\n",
       "                                         description  \n",
       "0  사장님들이 당근알바를 이용해 동네에서 빠르게 필요한 일손을 구하고 당근알바가 준비한...  \n",
       "1  대출잔액  조원 사상 최대 기록 경신  금융 연체율   상승 저축은행   최근 수년...  \n",
       "2    글로벌 바이오 인력양성 허브는  코로나 를 계기로 지역별 백신 바이오의약품 생산...  \n",
       "3  한편 김승학 목사는 이번 포럼과 관련해  배위량 선교사의 첫 발걸음이 단순히 교회 ...  \n",
       "4  그러나  년전에는  코로나 가 퍼지고 경제가 침체된  년이었고  행사의 부진은 당연...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cac3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분석에 사용할 클래스 객체 생성\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f82e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 형태소 토큰화\n",
    "def okt_tokenizer(text):\n",
    "    tokens = okt.morphs(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7370c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF - IDF 기반 벡터화를 위한 사이킷런 객체 생성\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tokenizer : 토큰 생성기\n",
    "# ngram_range: 토큰의 단어 크기\n",
    "# min_df: 토큰의 최소 출현 빈도\n",
    "# max_df: 최대 빈도(퍼센트)\n",
    "tfidf = TfidfVectorizer(tokenizer=okt_tokenizer, ngram_range=(1, 2), min_df=3, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3524a81f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_title_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m data_title_predict \u001b[38;5;241m=\u001b[39m SA_lr_best\u001b[38;5;241m.\u001b[39mpredict(data_title_tfidf)\n\u001b[0;32m      5\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_title_predict\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2099\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents):\n\u001b[0;32m   2084\u001b[0m     \u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2085\u001b[0m \n\u001b[0;32m   2086\u001b[0m \u001b[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2099\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2101\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1222\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1217\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1218\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1219\u001b[0m     ]\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "data_title_tfidf = tfidf.transform(data_df['title'])\n",
    "\n",
    "data_title_predict = SA_lr_best.predict(data_title_tfidf)\n",
    "\n",
    "data_df['title_label'] = data_title_predict\n",
    "data_description_tfidf = tfidf.transform(data_df['description'])\n",
    "\n",
    "data_description_predict = SA_lr_best.predict(data_description_tfidf)\n",
    "\n",
    "data_df['description_label'] = data_description_predict\n",
    "# csv 파일로 저장 ---------------------------------------------\n",
    "data_df.to_csv('./data/코로나new_label.csv', encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22026220",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['title','title_label','description','description_label']\n",
    "NEG_data_df = pd.DataFrame(columns=columns_name)\n",
    "POS_data_df = pd.DataFrame(columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in data_df.iterrows(): \n",
    "    title = data[\"title\"] \n",
    "    description = data[\"description\"] \n",
    "    t_label = data[\"title_label\"] \n",
    "    d_label = data[\"description_label\"] \n",
    "    \n",
    "    if d_label == 0: # 부정 감성 샘플만 추출\n",
    "        NEG_data_df = NEG_data_df._append(pd.DataFrame([[title, t_label, description, d_label]],columns=columns_name),ignore_index=True)\n",
    "    else : # 긍정 감성 샘플만 추출\n",
    "        POS_data_df = POS_data_df._append(pd.DataFrame([[title, t_label, description, d_label]],columns=columns_name),\n",
    "# 파일에 저장.\n",
    "NEG_data_df.to_csv('./data/코로나_news_NES.csv', encoding='euc-kr') \n",
    "POS_data_df.to_csv('./data/코로나_news_POS.csv', encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ab879",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(NEG_data_df), len(POS_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b129ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_description = POS_data_df['description']\n",
    "\n",
    "POS_description_noun_tk = []\n",
    "\n",
    "for d in POS_description:\n",
    "    POS_description_noun_tk.append(okt.nouns(d)) #형태소가 명사인 것만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_description_noun_join = []\n",
    "\n",
    "for d in POS_description_noun_tk:\n",
    "    d2 = [w for w in d if len(w) > 1] #길이가 1인 토큰은 제외\n",
    "    POS_description_noun_join.append(\" \".join(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tfidf = TfidfVectorizer(tokenizer = okt_tokenizer, min_df=2 )\n",
    "POS_dtm = POS_tfidf.fit_transform(POS_description_noun_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_vocab = dict() \n",
    "\n",
    "for idx, word in enumerate(POS_tfidf.get_feature_names()):\n",
    "    POS_vocab[word] = POS_dtm.getcol(idx).sum() # 단어별 TF-IDF 합\n",
    "    \n",
    "POS_words = sorted(POS_vocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0306bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6663f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams, style\n",
    "style.use('ggplot')\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "max = 15  #바 차트에 나타낼 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(max), [i[1] for i in POS_words[:max]], color=\"blue\")\n",
    "plt.title(\"긍정 뉴스의 단어 상위 %d개\" %max, fontsize=15)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"TF-IDF의 합\", fontsize=12)\n",
    "plt.xticks(range(max), [i[0] for i in POS_words[:max]], rotation=70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991363ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_description = NEG_data_df['description']\n",
    "\n",
    "NEG_description_noun_tk = []\n",
    "NEG_description_noun_join = []\n",
    "\n",
    "for d in NEG_description:\n",
    "    NEG_description_noun_tk.append(okt.nouns(d)) #형태소가 명사인 것만 추출\n",
    "    \n",
    "for d in NEG_description_noun_tk:\n",
    "    d2 = [w for w in d if len(w) > 1]  #길이가 1인 토큰은 제외\n",
    "    NEG_description_noun_join.append(\" \".join(d2)) # 토큰을 연결(join)하여 리스트 구성\n",
    "NEG_tfidf = TfidfVectorizer(tokenizer = okt_tokenizer, min_df=2 )\n",
    "NEG_dtm = NEG_tfidf.fit_transform(NEG_description_noun_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_vocab = dict() \n",
    "\n",
    "for idx, word in enumerate(NEG_tfidf.get_feature_names()):\n",
    "    NEG_vocab[word] = NEG_dtm.getcol(idx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(NEG_tfidf.get_feature_names()):\n",
    "    NEG_vocab[word] = NEG_dtm.getcol(idx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_words = sorted(NEG_vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "plt.bar(range(max), [i[1] for i in NEG_words[:max]], color=\"red\")\n",
    "plt.title(\"부정 뉴스의 단어 상위 %d개\" %max, fontsize=15)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"TF-IDF의 합\", fontsize=12)\n",
    "plt.xticks(range(max), [i[0] for i in NEG_words[:max]], rotation=70)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
